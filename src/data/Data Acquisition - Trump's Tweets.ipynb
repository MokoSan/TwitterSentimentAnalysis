{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datasets we will be dealing with are the following:\n",
    "\n",
    "1. __Trump's Tweets__: Starting from late 2016 till April 2018. \n",
    "2. __S&P Movements__: The movements of the S&P within the same time period. \n",
    "\n",
    "This notebook deals with acquiring and transforming __Trump's Tweets__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Trump's Tweets\n",
    "import re\n",
    "import tweepy\n",
    "import csv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer    \n",
    "\n",
    "# For S&P 500 Data\n",
    "from pandas import DataFrame\n",
    "import pandas_datareader.data as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump's Tweets Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that contains the user credentials to access Twitter API \n",
    "ACCESS_TOKEN        = 'Nothing'\n",
    "ACCESS_TOKEN_SECRET = 'to'\n",
    "CONSUMER_KEY        = 'see'\n",
    "CONSUMER_SECRET     = 'here.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(): \n",
    "\toauth = tweepy.OAuthHandler( CONSUMER_KEY, CONSUMER_SECRET )\n",
    "\toauth.set_access_token( ACCESS_TOKEN, ACCESS_TOKEN_SECRET ) \n",
    "\treturn oauth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting All Tweets for Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 tweets downloaded so far.\n",
      "600 tweets downloaded so far.\n",
      "800 tweets downloaded so far.\n",
      "1000 tweets downloaded so far.\n",
      "1200 tweets downloaded so far.\n",
      "1400 tweets downloaded so far.\n",
      "1600 tweets downloaded so far.\n",
      "1800 tweets downloaded so far.\n",
      "2000 tweets downloaded so far.\n",
      "2200 tweets downloaded so far.\n",
      "2400 tweets downloaded so far.\n",
      "2600 tweets downloaded so far.\n",
      "2800 tweets downloaded so far.\n",
      "3000 tweets downloaded so far.\n",
      "3200 tweets downloaded so far.\n",
      "3400 tweets downloaded so far.\n",
      "3600 tweets downloaded so far.\n",
      "3800 tweets downloaded so far.\n",
      "4000 tweets downloaded so far.\n",
      "4200 tweets downloaded so far.\n"
     ]
    }
   ],
   "source": [
    "def get_all_tweets( screen_name, total_tweets = 4000 ):\n",
    "    api = tweepy.API( auth() )\n",
    "    all_tweets = []\n",
    "\n",
    "    # Get the 200 Most Recent Tweets.\n",
    "    new_tweets = api.user_timeline( screen_name = screen_name, \n",
    "                                    count       = 200 )\n",
    "    all_tweets.extend( new_tweets )\n",
    "    \n",
    "    # Save the id of the oldest tweet less one\n",
    "    oldest_tweet_id = all_tweets[ -1 ].id - 1\n",
    "    \n",
    "    tweet_counter = 0\n",
    "\n",
    "    # Let's get the most recent 4000 tweets. \n",
    "    while tweet_counter < total_tweets:\n",
    "        \n",
    "        # All Subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline( screen_name = screen_name, \n",
    "                                        count       = 200,\n",
    "                                        max_id      = oldest_tweet_id, \n",
    "                                        tweet_mode  = 'extended' ) \n",
    "        tweet_counter += 200\n",
    "        \n",
    "        all_tweets.extend( new_tweets )\n",
    "        \n",
    "        # Update the id of the oldest tweet less one\n",
    "        oldest = all_tweets[ -1 ].id - 1\n",
    "        \n",
    "        print( f'{ len(all_tweets)} tweets downloaded so far.') \n",
    "        \n",
    "    return all_tweets\n",
    "\n",
    "donald_tweets = get_all_tweets( 'realDonaldTrump' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Looks like OPEC is at it again. With record amounts of Oil all over the '\n",
      " 'place, including the fully loaded ships at… https://t.co/l5MMjmtI14',\n",
      " 'Nancy Pelosi is going absolutely crazy about the big Tax Cuts given to the '\n",
      " 'American People by the Republicans...got… https://t.co/0REgmJNMqT',\n",
      " 'So exciting! I have agreed to be the Commencement Speaker at our GREAT Naval '\n",
      " 'Academy on May 25th in Annapolis, Mary… https://t.co/L9iZ6RS3ft',\n",
      " 'So General Michael Flynn’s life can be totally destroyed while Shadey James '\n",
      " 'Comey can Leak and Lie and make lots of… https://t.co/q1lyKyyeYI',\n",
      " 'James Comey Memos just out and show clearly that there was NO COLLUSION and '\n",
      " 'NO OBSTRUCTION. Also, he leaked classif… https://t.co/YfMYBrTkza']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Let's print the text of the first 5 tweets.\n",
    "pprint.pprint( [ t.text  for t in donald_tweets[ : 5 ]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranforming the Tweets Using Sentiment Analysis via NLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Functions__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment( input_tweet ):\n",
    "    return sid.polarity_scores( input_tweet )\n",
    "\n",
    "def clean_tweet( input_tweet ):\n",
    "    return ' '.join(re.sub( \"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\n",
    "                            \" \", \n",
    "                            input_tweet ).split() )\n",
    "\n",
    "def get_tweet_text( input_tweet ):\n",
    "    text = \"\"\n",
    "\n",
    "    if hasattr( input_tweet, 'full_text' ):\n",
    "        return clean_tweet( input_tweet.full_text )\n",
    "\n",
    "    elif hasattr( input_tweet, 'fulltext' ):\n",
    "        return clean_tweet( input_tweet.fulltext )\n",
    "\n",
    "    elif hasattr( input_tweet, 'text' ):\n",
    "        return clean_tweet( input_tweet.text )\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tranformation Logic__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tweets( input_tweets ):\n",
    "    output_tweets = [] \n",
    "        \n",
    "    for t in donald_tweets:\n",
    "        compound_value = 0\n",
    "        text           = get_tweet_text( t )\n",
    "        \n",
    "        if ( text == None ):\n",
    "            print( f'No text found for: User: {t.user.name} Tweet @ {t.created_at}')\n",
    "            continue\n",
    "        else:\n",
    "             compound_value = analyze_sentiment( text )[ 'compound' ] \n",
    "            \n",
    "        output_tweets.append( [ t.created_at, text, compound_value ])\n",
    "    return output_tweets\n",
    "            \n",
    "transformed_tweets = transform_tweets( donald_tweets ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets( input_tweets, file_path ):\n",
    "\n",
    "    with open( file_path, 'w' ) as csv_file:\n",
    "        csv_writer = csv.writer( csv_file )\n",
    "        csv_writer.writerow( [\"Created At\", \"Cleaned Tweet\", \"Sentiment Score\"])\n",
    "        csv_writer.writerows( input_tweets )\n",
    "        \n",
    "write_tweets( transformed_tweets, '../../data/realDonaldTrump_tweets.csv' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
